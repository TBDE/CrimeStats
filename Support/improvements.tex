As the program stands; it finds an optimal position for the relocatable precincts, where optimal means the sum of the distances between a crime location and it ``assigned'' precinct location is minimized across all crimes and precincts.  There are several implied attributes in this minimization, including:
\begin{enumerate}
\item The correct distance measurement is being used.  At its surface, how to measure distance can seem a trivial problem, but it is very interesting because because to measure the distance between two (or more) things, a common set of reference points has to be established.  The ``distance'' between two things, can also be called their ``dissimilarity''\cite{deza2009encyclopedia}.  In the current R script, a haversine (or great circle) distance is used because both the crime and precinct data have latitude and longitude values.  Alternatively, a Euclidean formula could be used, because in general the range distances is small and the associated errors are probably acceptable.  A possibly better distance measurement would be the Manhattan distance because it takes into account streets and directionality, vice being ``as the crow flies.''  Haversine was chosen because it was felt that making HTTP API calls for each crime and each precinct would take too long.
\item Any crime can be assigned to any precinct.  It is easy to envision that crimes of different classifications could be assigned to precincts with expertise in handling that classification.  This ``specialization'' would very likely affect the location of the movable precincts.
\item It is assumed that the precinct can exist at its optimal location.  The computed location does not address whether or not a precinct could be constructed at the optimal location.  If the optimal location is in a lake, or river, the placement program has no knowledge of those types of restrictions.
\item The optimal location is computed based on the positional crime data used.  Each of the crime reports has a time and date component.  The current application does not use these data.  It is possible that the optimal location could change over time.  This is an aspect of the data that has not been addressed.
\item The optimal location is based on crime locations, but looking at the crime data on a geographic plot, seems to indicate that there is more crime in densely populated areas.  It may be that there is a relatively constant crime rate, and therefore precincts should be placed in the more densely populated areas because there will be more crimes nearby.
\item Using a more refined way to detect when a solution has been detected.  Currently the search for a solution terminates when either one of two conditions are met.  Either the maximum number of iterations has been reached, or the current sum of distances is EXACTLY the same as the previous sum of distances.  The maximum number of iterations threshold is an escape if the system becomes unstable for some reason.  A change in how the sum of distances is computed, or the change in the sum is detected could terminate the iterations earlier.  One possible approach is to use an exponential average of the sum of distances to detect stability, where the current sum is compared to the exponentially average sum to detect 0 change.  Another approach is to establish a percentage change between the previous value and the current value that would signify stability.  These ideas come from looking at the long tail of the closure rate plot.
\item During the crime to precinct allocation process, occasionally all crimes assigned to a precinct will be reassigned to a nearer precinct.  When this happens, the precinct is in a sense ``orphaned.''  Because the goal is to place a preset number of precincts, when a precinct is orphaned, another attempt is made to place it in a ``good'' location.  the standard deviation of all crimes from their assigned precincts is computed, and the precinct with the highest standard deviation is identified.  Using the location of this precinct, the crime that is furthest away is identified (if there is more than one crime at the same distance, the first one is used).  A point midway between the precinct with the highest standard deviation, and that precinct's furthest crime is the candidate location for the unused precinct.  An improvement could be to use a midpoint of all existing precincts (a sort of spring force position), the argument that this new location would have the greatest benefit to the entire system vice just one precinct.  Another variation would be to use the precinct with the highest standard deviation, its furthest crime, and the location of the nearest precinct that it was not assigned to as the spring locations.  This approach would provide the highest benefit to a portion of the system.  The current system does not handle the situation where precincts are ``orphaned'' more than once.
\item Currently the program focuses on placing the precincts at locations that minimizes the distances between all crimes and all precincts.  An alternative goal could be to minimize the cost of the system.  A precinct could have a fixed cost, and each assigned crime could be assigned a cost (representing personnel, material, and other direct and indirect expenses).  The goal could then become to minimize the cost of the total system.
\item Precinct locations/movements could be monitored to determine system stability.  If the changes in precinct location across the board were to fall below some threshold (say 0.002 degrees, or approximately 0.1 miles), then it could be assumed that the system had reached a nearly stable condition.
\item The bulk of the current program's execution time is spent computing the distance between each crime and each precinct.  The crime location data is invariant, while at least some of the precincts' location data will vary.  A possible way to improve performance would be to have a collection of servers that would return crime to precinct assignments when given an updated set of precinct locations.  In theory, if the precinct data were spread across 4 servers, then execution time would be reduced by approximately a factor of two thirds (allowing for overhead time to marshal and unmarshal results).  This might be a really big boost as the number of crimes, or precincts increases.
\item The current implementation injects new precincts one at a time.  A variation would be to inject more than one at a time and see how the system performs.  Perhaps all precincts would be used, and there wouldn't be any ``orphans.''
\item Consolidate police reports.  Cursory examination of some of the reports seems to indicate that multiple reports were done at the same location simultaneously.  Therefore they could be considered to be the same incident, vice separate incidents.  The threshold for consolidation should consider time within some small tolerance, location within some small tolerance, and possibly the type of crime.
\item Remove context switches to improve execution time.  It appears that R uses a ``pass by value'' model when passing arguments to functions.  This is supported by the fact that changes made in a function to data it was passed, are not visible in the calling function.  The marshalling of arguments to pass to a function has to take time, this time could be eliminated if there was only one context.  Rewriting the script to eliminate functions would decrease its maintainability, but should reduce execution time.
  \item Allow the program to be restarted after successful conclusion.  Currently most of the program's ``state'' data is persisted to support post run analysis.  The program could be restructured to take advantage of this data so that runs with higher number of precincts could use previous data as a starting point.
\end{enumerate}
